--- 
title: "Hacking for Social Scientists"
subtitle: "A Guide to Programming With Data"
author: "Matthias Bannert"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
css: ["h4sci.css"]
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "The vast majority of data has been created within the last decade. In turn many fields of research are confronted with an unprecented wealth of data. The sheer amount of information but also the complexity of modern datasets continues to point researchers to programming approaches who had not considered programming to process data so far. Hacking for Social Sciences and Humanities"
---

> "There two things users hate,
> *change*, and the way things are."
> `r tufte::quote_footer('--- \\@TheWierdWorld (ShowerThoughts Twitter Account)')`

# Preface

The vast majority of data has been created within the last decade. In turn many fields of research are confronted with an unprecented wealth of data. The sheer amount of information but also the complexity of modern datasets continues to point a kind researcher to programming approaches who had not considered programming to process data so far. Hacking for Social Sciences and Humanities aims to give a big picture overview and starting point to reach what the open source software community calls a 'software carpentry' level. Also, this book argues a solid software carpentry skill level is totally in reach for most researchers. And most importantly, investing is worth the effort: being able to code leverages field specific expertise and fosters interdisciplinary collaboration as source code continues to become an important communication channel. 

# Introduction - The Choice that Doesn't Matter

The very first (and intimidating) choice a novice hacker faces is which is programming language to learn. Unfortunately the medium popularily summed up as the internet offers a lot of really really good advice on the matter. The problem is, however, that this advice does not necessarily agree which language is the best for research. In the realm of data science -- get accustomed to that label if you are a scientist who works with data -- the debate basically comes down to two languages: The R Language for Statistical Computing and Python. 

At least to me, there is only one valid advice: **It simply does NOT matter**. If you stick around in data science long enough you will eventually get in touch with both languages and in turn learn both. There is a huge overlap of what you can with both languages. R came out of the rather specific domain of statiscs 25+ years ago and made its way to a more general programming language thanks to 15K+ extension packages (and counting). Built by a mathmatician Python continues to be as general purpose as ever but got more scientific thanks to packages such as {pandas}, {sciPy} or {numPy}. As a result there is a huge overlap of what both languages can do and both will extend your horizon in unprecendented fashion if you did not use a full fledged programming language for your analysis before. 

But why is there such a heartfelt debate online, if it doesn't matter? Let's pick up a random argument from this debate: R is to set up and Python is better for machine learning. If you worked with Java or another environment that's rather tricky to get going, you are hardened and might not need or cherish easy onboarding. If you got frustrated before you even really started trying to reproduce a nifty machine learning blog post because you installed the wrong version of Python or didn't manage to make sense of virtualenv you might feel otherwise. 

The point is,  rest assured, if you just start doing analytics using a programming languages both languages are guaranteed to carry you a long way. There is no way to tell for sure which one will be the more dominant language in 10 years from now or whether both still be around holding their ground the way they do now. But once you reached a decent software carpentry level in either language it will help you a lot learning the other. If your peers work with R, start with R, if your close community
works with Python, start with Python. If you are in for the longer run either language will help you understand the concepts and ideas of programming with data. Trust me, there will be a natural opportunity to get to know the other. 


## Why Would Social Scientists Want to Code?

First of all, because everybody and their grandmothers seem to do it. Statistical computing continues to be on the rise in many branches of social sciences. 



<!-- The below graph shows monthly accumulated extension package downloads for the R Language of Statistical Computing grouped by fields of research^[proxied this and that].
Econometrics, Psychometrics, National Language Processing, Official Statistics, Social Sciences

```{r, eval=TRUE}
# add CRAN TASK VIEW Download 
# Statistics graph here
rnorm(10)
```

-->

Second because it's reproducible. Code has become a tremendous communication channel. Your web scraper does not work? 
Instead of reaching out in a clumsy but wordy cry for help, posting what you tried to far described by source code will often get you good answers within hours on platforms like [Stackoverflow](https://stackoverflow.com) or [Crossvalidated](https://crossvalidated.com). Or imagine feature requests, after a little code ping pong with the package author your wish eventually becomes clearer. Let alone chats with colleagues and co-authors. Sharing code just works. 
Academic journals have found out, too in the meantime. Many outlets require you to make the data and source code behind your work available. [Social Science Data Editors](https://social-science-data-editors.github.io/guidance/template-README.html) is a bleeding edge project at the time of reading this, but is already referred to by top notch journals of the profession like American Economic Review (AER). 

Third, because it scales and automates. Automation is not only convenient. Like when you want to download data, process and create the same visualization and put it on your website any given Sunday. Automation is inevitable. Like when you have to gather daily updates from different outlets or work through thousands of .pdfs. 

Last but not least because of things you couldn't do w/o being an absolute guru (if at all) if wasn't for programming. Take  visualization. Go, check [these D3 Examples](https://d3js.org/). Now, try to do that in Excel. If you do these things in Excel it'd make you an absolute spreadsheet visualization Jedi, probably missing out on other time consuming skills to master. Moral of the story is, with decent, carpentry level programming skills -- that'd be the upfront investment -- you can already do so many spectular things while not really specializing and staying very flexible. 




## How to Read this Book? 

*Hacking for Social Sciences* is written based on the experience of helping students and seasoned researchers of different fields with their data management, processing and communication of results. A part of the book contains the information I wish I had when I started a PhD in economics. Part of the book is written years after said PhD was completed and with the hindsight of 10+ years in academia. Every page of the book is written with the belief that the future is OPEN and it is up to our generation of researchers to shape it. 

<!-- add ad from Germany "the future is OPEN" for open data and open source 
https://www.bildung-forschung.digital/files/191004_OA-Infoflyer_barrierefrei.pdf
-->

>If you came to ```r emo::ji("cherry")``` pick, you're welcome, too (but a bit early to the party though). This book will grow along the 2020 course 'Hacking for Social Science' and hopefully be finished in its first version by the end of the semester / year. Next up are chapters on the *Big Picture of Open Source Software for hacking data* and *Git version control*. 


# Stack - A Developer's Toolkit

Just like natural craftsmen, digital carpenters depend on their toolbox and their mastery of it. 
*Stack* is what developers call the choice of tools used in a particular project. Even though different flavors come down to personal preferences, there is a lot of common ground in *programming with data* stacks. Throughout this book, often a choice for one piece of software needs to be made in order to illustrate things. But please do understand these choices as examples and focus on the role of an item in the bigger picture. To help you with the big picture of which tool does what the following section will group common programming-with-data stack components. Also, note notice that not every role has to be filled in every project. 

<img src="images/dr_egghead_panics.jpg" width="700px">
<div class="caption">Aaaaaaah! Dr. Egghead is overwhelmed by the plethora of tools.</div>

Don't panic, Dr. Egghead! All these components are here to help you and there is plenty of choices...
Here are the components I use most often. Note, this is a personal choice which works for me. Obviously not ALL of these are components are used in every small project. *Git, R* and *R Studio* would be the very minimal version, I guess. 

```{r, eval=TRUE,message=FALSE, echo=FALSE}
library(kableExtra)
d <- data.frame(Component = c("Interpreter / Language", "IDE / Editor",
                              "Version Control","Project Management",
                              "Database", "'Virtual' Environments", "Website Hosting",
                              "Workflow Automation", "Continous Integration"),
                Choice = c("<a href='https://r-project.org'>R</a>, <a href='https://www.python.org/'>Python</a>, <a href=''>Javascript</a>","<a href='https://rstudio.com'>R Studio</a>,<a href='https://code.visualstudio.com/'>VS Code</a>, <a href='https://www.sublimetext.com/'>Sublime</a>",
                           "<a href='https://git-scm.com/'>Git</a>", "<a href='https://github.com/features/project-management/'>GitHub</a>, <a href='https://about.gitlab.com/solutions/project-management/'>GitLab</a>","<a href='https://www.postgresql.org/'>PostgreSQL</a>","<a href='https://www.docker.com/'>Docker</a>",
                           "<a href='https://netlify.com/'>Netlify</a>, <a href='https://pages.github.com/'>GitHub Pages</a>","<a href='https://airflow.apache.org/'>Apache Airflow</a>", "<a href='https://docs.gitlab.com/ee/ci/'>GitLab CI</a>"),
                stringsAsFactors = FALSE)
kable(d,"html",escape= FALSE)

```


## Languages: Compiled vs. Interpreted

In Statistical Computing -- at least in Social Sciences -- the interface between the researcher and the computation node is almost always an interpreted progamming language as opposed to a compiled one. Compiled languages like C++ require the developer to write source code and compile, i.e. translation of the source code into what a machine can work with happens *before* runtime. The result of the compilation process is a binary which is specific to the operating system. Hence you will need a version for Windows, one for OSX and one for Linux if you intend to reach a truly broad audience with your program. 
The main advantage of a compiled language is speed in terms of computing performance as the translation into machine language does not happen during runtime. A reduction of development speed and increase in required developer skills are the downside of using compiled languages. 

> Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it. </br></br>
-- Dan Ariely, Professor of Psychology and Behavioral Economics, <a href="https://twitter.com/danariely/status/287952257926971392">on twitter</a>

The above quote became famous in the hacking data community, not only because of the provocative, fun part of it, but also because of the implicit advice behind it. Given the enormous gain in computing power in recent decades, but also methodological advances, interpreted languages are often fast enough for many social science problems. And even if it turns out, your data grow out of your setup, a well written proof of concept written in an interpreted language can be a formidable blueprint. **Source code is turning into an important scientific communication channel.** Put your money on it, your interdisciplinary collaborator from the High Performance Computing (HPC) group, will prefer some Python code as a briefing for their C++ or FORTRAN program over a wordy description out of your field's ivory tower. 

Interpreted language are a bit like pocket calculators, you can look at intermediate results, line by line. 
R and Python are the most popular OSS choices in hacking with data, <a href="https://julialang.org/">Julia</a> is an up and coming, perfomance focused language with a much slimmer ecosystem. A bit of Javascript can't hurt for advanced customization of graphics and online communication of your results.


<!-- Julia screenshot with caption -->


## IDE

It's certainly possible to move a four person family into a new home by public transport, but it is not convenient. The same holds for (plain) text editors in programming. You can use them, but most people would prefer an Integrated Development Environment (IDE) just like they prefer to use a truck when they move. IDEs are tailored to the needs and idiosyncrasies of a language, some working with plugins and covering multiple languages. Others have a specific focus on a single language or a group of languages. Here are some of the features you are looking for in an IDE for programming with data

- [code highlighting](https://en.wikipedia.org/wiki/Syntax_highlighting), linting
- decent file explorer
- terminal integration
- git integration
- markdown support
- debugging tools
- build tools
- customizable through add ins / macros


For R, the Open Source Edition of [R Studio Desktop](https://rstudio.com/products/rstudio/) is the right choice for most people.
(If you are working in a team, R Studio's server version is great. It allows to have a centrally managed server which clients can use through their a web browser without even installing R and R Studio locally.) R Studio has solid support for a few other languages often used together with R, plus it's customizable. The French premier thinkR [Colin_Fay]() gave a nice tutorial on [Hacking R Studio](https://speakerdeck.com/colinfay/hacking-rstudio-advanced-use-of-your-favorite-ide) at the useR! 2019 conference.

<img src="images/rstudio_r_py.png" width="350px">
<div class="caption-half">Screenshot of the coverpage of the R Studio <a href="https://rstudio.com">website</a> in fall 2020. The site advertises R Studio as an IDE for both languages R and Python. Remember <a href="">The Choice that Doesn't Matter?</a></div>

While R Studio managed to hold its ground among R aficionados as of fall 2020, Microsoft's free *Visual Studio Code* has blown the competition out of the water otherwise. Microsoft's IDE is blazing fast, extendable and polyglot. [VS Code Live Share](https://visualstudio.microsoft.com/services/live-share/) is just one rather random example of its remarkably well implemented features. Live share allows developers to edit a document simultaneously using multiple cursors in similar fashion to Google Docs, but with all the IDE magic and in a Desktop client. 

Another approach is to go for a highly customizable editor such as *Sublime* or *Atom*. 
The idea is to send source code from the editor to interchangeable REPLs (read-eval-print-loops)
which can be adapted according to the language that needs to be interpreted. That way a good linter / code highlighter for your favorite language is needed for the editor and you have a lightweight environment to run things. An example of such a customization approach is Christoph Sax OSS small project [Sublime Studio](https://github.com/christophsax/SublimeStudio). 

Other examples for popular IDEs are Eclipse (mostly Java but tons of plugins for other languages), IntelliJ (Java) and PyCharm (Python). 


## Version Control

To buy into the importance of managing one's code professionally may be the single most important take away from *Hacking for Social Sciences*. Being able to work with version control will help you fit into a lot of different teams that have contact points with data science and programming, let alone if you become part of a programming or data science team. 

While version control has a long history dating back to CVS and SVN, the good news for the learner is, that there is a single dominant approach when it comes to version control in academia. Despite the fact that it's predecessors and alternative such as mercurial are stil around, git is the one you have to learn. To learn more about the history of version controls and approaches other than git, [Eric Sink's Version Control by Example](https://ericsink.com/vcbe/index.html) 
is for you. 

So what does git do for us as researchers? How is it different from dropbox?


```
git does not work like dropbox. git does not work like dropbox.
git does not work like dropbox. git does not work like dropbox. 
git does not work like dropbox. git does not work like dropbox.
git does not work like dropbox. git does not work like dropbox. 
git does not work like dropbox. git does not work like dropbox.
git does not work like dropbox. git does not work like dropbox. 

```

The idea of thinking of a sync, is what hampers from understanding the benefit of right away (which why I hate that git [GUI]()s called it 'sync' anway). Git is a decentralized version control system that keeps track of a history of semantic commits that may consists of changes to multiple files.
A commit message summarizes the gist of a contribution bundles a contribution. *Diffs* allow to changes between different versions. 

<img src="images/diff.png" width="300px">
<div class="caption-half">The *diff* output shows an edit during the writing of this book. The line preceeded by '-' was replaced with the line preceeded by '+'.</div>

Git is well suited for any kind of text file / source code from Python or C++ to markdown or LaTeX. Binaries like .pdfs or Word documents are possible, too, but certainly not the type of files for which git is really useful. This book contains a detailed, applied introduction tailed to researchers as part of the *Programmers' Practices and Workflows* chapter, so let's dwell with the above contextualization for a bit.   


## Database: Relational vs. Non-Relational 

To evaluate which database to pick up just seems like the next daunting task of stack choice. Luckily, in research first encounters with a database are usually passive, in the sense that you want to query data from a source that uses a particular database. So unless you want to start your own data collection from, simply sit back, relax and let the internet battle out another conceptual war. 

Database Management Systems (DBMS) are basically grouped into [relational]() and [non-relational]() ones. Relational databases with their Structured Query Language (SQL) have been around forever. SQL became and ISO and ANSI standard and continue to be essence of many many backends around the world. Oracle continues to be the benchmark for SQL databases but opensource PostgreSQL and Microsoft's SQL Server operate at eye level for many applications.
MySQL, Oracle's slim, little (but free) brother, can't quite cope with PostgreSQL, continues to be the most used SQL database on the planet. This is mainly due to its popularity for web applications like the blogging [CMS]() [wordpress](). Last but not least, *sqlite* needs to be mentioned when talking about relational database. The name nutshells its concept quite well: It's an easy to use, much simpler version of the aforementioned database management systems. It is extremely popular for light but powerful applications that want to organize data with a SQL approach in a single file (mobile applications like to use it for example).

No-SQL databases are the anti establishment, anti standard approach. [MongoDB](https://www.mongodb.com) may be the best marketed among the rebels.Before you start to sympathize with latter approach because the wording of my last to sentences, let's stop here. Large infrastructure players make the case for non-relational stores like CouchDB or Amazon Redshift Database, but trust me, you those are unlikely the first things you get to run when your research grows out of Excel. If your are not happy with the 'beyond-the-scope-of-this-book' argument, blogging experts like [Lukas Eder](https://blog.jooq.org/tag/nosql/) maybe biased but much better educated to educate you here. The idea of this chapter is just to help you group all the database stores you might face to soon as a researchers. 

The good news is, languages like R and Python are so well equipped to interface with a plethora of databases. So well that I often recommend these languages to researcher who work with other less equppied tools, solely as an intermediate layer. And if there is really no database extension around for your language, a general [ODBC](https://en.wikipedia.org/wiki/Open_Database_Connectivity) interface helps -- at least for SQL databases.  




## Single Purpose Environments

Though a bit out of fashion and somewhat different, virtual machines are good starting point to explain single purpose container environments such as [docker](https://www.docker.com/why-docker). A virtual machine (VM) is basically a computer in a computer, like a Linux environment running on your Windows notebook. Oracle's free [Virtual Box](https://www.virtualbox.org/) is the most popular piece of software to easily install another operating system inside your local computer's host. 

While VMs are still common and one can potentially have lots of images for different purposes, images are too heavyweight and take to long to boot to be the go-to solution for many application developers. Hence so-called containers that run within a container host and fire up within seconds have become popular as single purpose environments.

The most popular of them all is *docker* which allows users to configure an environment in a *Dockerfile* (essentially a text file) including the operating system and software packages installed in the container. The text file can either be used to create a *docker image* which is kind of a blueprint for a container. Containers run inside a docker host and can either be used interactively or in a batch which executes a single task in an environment specifically built for this task.  

One of the reasons why docker is attractive to researchers is its open character: Dockerfiles are a good way to share a configuration in a simple, reproducible script, making it easy to reproduce. Less experienced researchers can benefit from [Dockerhub](https://hub.docker.com/) which shares images for a plethora of purposes from mixed data science setups to database configuration. Side Effect free working environments for all sorts of task can especially be appealing to developers with limited experience in system administration. 

Beside simplification of system administration, *docker* is known for its ability to work in the cloud. All major cloud hosters offer docker hosts and the ability to deploy docker containers that were previously developed and tested locally in the cloud. You can also use docker to tackle throughput problems using container orchestration tools like [Docker Swarm](https://docs.docker.com/engine/swarm/swarm-tutorial/) or [K8 (say: Kubernetes)](https://kubernetes.io/) to run hundreds of containers (depending on your virtual resources).





## Communication

Communication is an essentially part of building an (academic) career. Part of it is a neat online profile. Do not relax on the excuse that are department's website does not offer the flexibility. The legal and technical situation in many places should allow you to spin up your own website or even run a blog if you find the time. For free. Including the web hosting. 

A popular approach to do so is to work with a *static website generator*. Generators like blogdown, pkgdown or bookdown are flavors of the same approach to create a website from markdown first and then upload rendered HTML + CSS + Javascript page to a host like GitHub Pages or Netlify that allow you to store it online without paying. 

The idea of engines like the [Go](https://golang.org/) based [Hugo](https://gohugo.io/) or the [Ruby](https://www.ruby-lang.org/en/) based [Jekyll](https://jekyllrb.com/) which are behind the above packages is a counter approach to what content management systems do [CMS](): There is no database and templates that are brought together dynamically when a user visits the website. This rendering is done locally on the creator's local notebook. Whenever a change is made, the website is rendered entirely (ok, minus caching) and uploaded (pushed) again to the host. Therefore no database, etc is needed on the webserver which cuts down the hosting costs to zero. (FWIW: this book is made with such a generator) 


## Automation

The first type of automation described here refers to automation of your development workflow. That it is, you standardize your path from draft to program to deployment of your program to production. Modern version control software accompanies this process with a toolchain that is often fuzzily called [CI/CD](https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment). While CI stands for *continuous integration* and simply refers to a workflow in which the team tries to release new features to production as continuously as possible, CD stands for either *continuous delivery* or *continuous deployment*.

However, in practice the entire toolchain referred to as *CI/CD* has become readily available in well documented fashion when git hosting powerhouses GitHub and GitLab introduced their flavors of it: [GitHub Actions])(https://docs.github.com/en/actions) and [GitLab CI](https://docs.gitlab.com/ee/ci/). In addition services like [Travis CI](https://travis-ci.org/) or [Circle CI](https://circleci.com/) offer this toolchain independently of hosting git repositories.

Users of these platforms can upload a simple textfile that follows a name convention and structure to trigger a step based toolchain based on an event. An example of an event may be the push to a repository's main branch. A common example would be to run tests and/or build process of a package and then upon success deploy the newly created package to some server -- all triggered by simple push to master. One particularly cool thing is, that there multiple services who allow to run the testing on their servers using container technologies to offer a plethora of setups for testing. This software can easily be tested on different operating systems or other dependencies. 


Here is a simple example of a *.gitlab-ci.yml* configuration that builds and tests a package and deploys it on push to master if the tests succeed:

```
stages:
- buildncheck
- deploy_pack

test:
image:
name: some.docker.registry.com/some-image:0.2.0
entrypoint:
- ""
stage: buildncheck
artifacts:
untracked: true
script:
- rm .gitlab-ci.yml # we don't need it and it causes a hidden file NOTE
- install2.r --repos custom.mini.cran.ch .
- R CMD build . --no-build-vignettes --no-manual
- R CMD check --no-manual *.tar.gz

deploy_pack:
only: 
- master
stage: deploy_pack
image:
name: byrnedo/alpine-curl
entrypoint: [""]
dependencies:
- 'test'
script:
- do some more steps to login and deploy to server ...

```

For more in depth examples of the above, [Jim Hester's talk on GitHub Actions for R](https://www.jimhester.com/talk/2020-rsc-github-actions/) is a very good starting point.


The other automation too I would like to mention is [Apache Airflow](https://airflow.apache.org/) because of its ability to help researchers keep an overview of regularly running processes. Examples of such processes could be daily or monthly data sourcing or timely publication of a regularly published indicator. For processes we can simply use [cronjobs](https://en.wikipedia.org/wiki/Cron) to time execution, but Airflow ships with a dashboard to keep track of many such process, plus a ton of other log and reporting features worth a lot when maintaining reocurring processe processes. 





# Developer Practices & Workflows

Just like most experienced engineers, seasoned software developers follow some kind of school or paradigm. Good programmers can even switch among approaches according to their current project's needs or depending on the team they are on. 

This section does not want to give a comprehensive overview over programming concepts nor compare approaches. And damn sure it does not mean to go to war over approach superiority. *Hacking for Social Scientists* rather cherry-picks suitable application-minded, low-barrier concepts that help social scientists professionalize their own programming. 


## Git Version Control 101

Version control may be the single most important thing to take away from *Hacking for Social Sciences*. 
In this chapter about the way developers work, I will stick to version control with *git*. The stack discussion of the previous chapter features a few more version control systems, but given git's dominant position, we will stick solely to git in this introduction to version control.  

### What is Git Version Control? 

Git is a decentralized version control system. It manages different versions of your source code (and other text files) in a simple put efficient manner that has become the industry standard: The git programm itself is a small console programm that creates and manages a hidden folder inside the folder you put under version control (you know those folders with a leading dot in their foldername, like .myfolder). This folder keeps track of all difference between the current version and other versions before the current one. 

<img src="images/commits.png" width="300px">
<div class="caption-half">Meaningful commit messages help to make sense of a project's history.</div>


The key to appreciate the value of git is to appreciate the value of semantic versions. Git is *not* Dropbox nor Google Drive. It does *not* sync automagically (even if some Git GUI Tools suggest so). Because these GUIs tools^[[GitHub Desktop](https://desktop.github.com/), Atlassian's [Source Tree](https://www.sourcetreeapp.com/) and [Tortoise](https://tortoisegit.org/) are some of the most popular choices if you are not a console person.] may be convenient but do not really help to improve your understanding of the workflow, we will use the git console throughout this book. As opposed to the sync approaches mentioned above, a version control system allows to summarize a contribution across files and folders based on what this contribution is about. Assume you got a cool pointer from an econometrics professor at a conference and you incorporated her advice in your work. That advice is likely to affect different parts of your work: your text and your code. As opposed to syncing each of these files based on the time you saved them, version control creates a version when you decide to bundle things together and to commit the change. That version could be identified easily by its commit message 'incorporated advice from Anna (discussion at XYZ Conf 2020)'. 


### Why Use Version Control in Research? 

>A version control based workflow is a path to your goals that rather consists of semantically relevant steps instead of semantically meaningless chunks based on the time you saved them. 

In other more blatant, applied words: naming files like 
`final_version_your_name.R` or `final_final_correction_collaboratorX_20200114.R` 
is like naming your WiFi `dont_park_the_car_in_the_frontyard` or `be_quiet_at_night` to communicate with your neighbors.  Information is supposed to be sent in a message, not a file name. With version control it is immediately clear what the most current version is - no matter the file name. No room for interpretation. No need to start guessing about the delta between the current version and another version.

Also, you can easily try out different scenarios on different branches and merge them back together if you need to. Version control is a well established industry standard in software development. And it is relatively easy to adopt. With datasets growing in size and complexity it is only natural to improve management of the code that processes these data. 

Academia is probably the only place would allow you to dive into hacking at somewhat complex problems for several years w/o ever taking notice of version control. As a social scientist who rather collaborates in small groups and writes moderate amount of code, have you ever thought about how to collaborate with 100+ persons big software project? Or you to manage ten thousands of lines of code and beyond? Version control is an important reason why these things work. And it's been around for decades. But enough about the rant... 


### How Does Git Work ? 

The first important implication of decentralized version control is that all versions are stored on the local machines of every collaborator, not just on a remote server. So let's consider a single local machine first.

<img src="images/decentralized.png" width="700px">
<div class="caption-half"></div>

Locally, a git repository consists of a *checkout* which is also called current *working copy* soon. This is the status of the file that your file explorer or your editor will see when you use them to open a file. To checkout a different version, one needs to find call a commit by its unique commit hash and checkout that particular version. 

If you want to add new files to version control or bundle changes to some existing files into a new commit, add these files to the staging area, so they get committed next time a commit process is triggered. Finally committing all these staged changes under another commit id a new version is created. 


### Moving Around

So let's actually do it. Here's a three stage walk through of git commands that should have you covered in most use cases a researcher will face. Note that git has some pretty good error message that guess what could have gone wrong, make sure to read them carefully. Even you can't make sense of them, your online search will be a lot more efficient when you include these messages. 

**Stage 1: Working Locally**

```{r, eval=TRUE,message=FALSE, echo=FALSE}
library(kableExtra)
d <- data.frame("Command" = c("git init",
                              "git status",
                              "git add file_name.py",
                              "git commit -m 'meaningful msg'",
                              "git log",
                              "git checkout some-commit-id",
                              "git checkout main-branch-name"),
                "Effect" = c("put current directory and all its subdirs under version control.",
                             "shows status",
                             "adds file to tracked files",
                             "creates a new version/commit out of all staged files",
                             "show log of all commit messages on a branch",
                             "go to commit, but in detached HEAD state",
                             "leave temporary state, go back to last commit"),
                
                stringsAsFactors = FALSE)
kable(d,"html",escape= FALSE)

```





**Stage 2: Working with a Remote Repository**

Though git can be tremendously useful even without collaborators, the real fun starts
when working together. The first step en route to get others involved is to add a remote repository. 


```{r, eval=TRUE,message=FALSE, echo=FALSE}
library(kableExtra)
d <- data.frame("Command" = c("git clone",
                              "git pull",
                              "git push",
                              "git fetch",
                              "git remote -v",
                              "git remote set-url origin https://some-url.com"),
                "Effect" = c("creates a new repo based on a remote one",
                             "get all changes from a linked remote repo",
                             "deploy all commit changes to the remote repo",
                             "fetch branches that were created on remote",
                             "show remote repo URL",
                             "set URL to remote repo"),
                stringsAsFactors = FALSE)
kable(d,"html",escape= FALSE)

```

**Stage 3: Branches**


```{r, eval=TRUE,message=FALSE, echo=FALSE}
library(kableExtra)
d <- data.frame("Command" = c("git checkout -b branchname",
                              "git branch",
                              "git checkout branchname",
                              "git merge branchname"),
                "Effect" = c("create new branch named branchname",
                             "show locally available branches",
                             "switch to branch named branchname",
                             "merge branch named branchname into current branch"
                ),
                stringsAsFactors = FALSE)
kable(d,"html",escape= FALSE)

```





**Fixing Merge Conflicts** 

In most cases git is quite clever and can figure out which is the desired state of 
a file when putting two versions of it together. When git's *recursive strategy* is at work, 
it will just merge versions automatically. When the same lines were affected in different versions, git cannot tell 
which line should be kept. Sometimes you would even want to keep both changes. 

But even in such scenario fixing the conflict is easy. Git will tell you that your last command
caused a merge conflict and which files are conflicted. Open these files and see all parts of the file that
are in question. 

<img src="images/merge_conflict.png">
<div class="caption-left">Ouch! We created a conflict by editing the same line in the same file on different branches and trying to but these branches back together.</div>

Luckily git marks the exact spot where the conflict happens. Good text editors / IDEs 
ship with cool colors to highlight all our options. 

<img src="images/sublime_conflict.png">
<div class="caption-left">go for the current status or take what's coming in from the a2 branch?</div>

Some of the fancier editors even have git conflict resolve plugins that let you walk through all conflict points. 

<img src="images/vscode_conflict.png">
<div class="caption-left">In VS Code you can even click the option.</div>

At the and of the day, all do the same, i.e., remove the unwanted part including all the marker gibberish. 
After you have done so, save, commit and push (if you are working with a remote repo) . Don't forget to make sure you kinked out ALL conflicts.  



## Feature Branches, PRs and Forks

This section discusses real world collaboration workflows of modern open source software developers. 
Hence the prerequisites are a bit different in order to benefit the most from this section. Make sure you are past
being able to describe and explain git basics, be able to create and handle your own repositories. 

If you had only a handful of close collaborators so far, you may be fine with staying on the main branch and trying
not to step on each others feet. This is reasonable because it is not useful to work asynchronously 
on exact the same lines of code anyway. Nevertheless, there is a reason why *feature-branch-based* workflows became 
very popular among developers: Imagine you collaborate less synchronously, maybe with someone in another timezone. Or with a colleague who works on your project, but in a totally different month during the year. Or, most obviously, we someone you have never met. Forks and *feature-branch-based* workflows is the way a lot of modern open source projects are organized. 

Forks are just a way to contribute via feature branches even in case you do not have write access to a repository. But let's just have look at the basic case in which you are part of the team first. Assume there is already some work done, some version of the project is already up on GitHub. You join as a collaborator and are allowed to push changes now. It's certainly not a good idea to simply add things without review to a project's production. Like if you got access to modify the institute's website and you made your first changes and all of a sudden the website looks like this:

<div class="yellow">&nbsp;</div>
<div id="pink">
It used to be subtle and light gray. I swear!
</div>

Bet everybody on the team took notice of the new team member by then. In a feature branch workflow you would start from the latest production version. Remember, git is decentralized and you have all versions of your team's project. Right at your fingertips on your local machine. Create a new branch named indicative of the feature you are looking to work on. 

```
git checkout -b colorways
```

You are automatically being switched to the freshly created branch. Do your thing now. It could be just a single commit, or several commits by different persons. Once you are done, i.e., commited all changes, add your branch to the remote repository by pushing.  

```
git push -u origin colorways
```

This will add a your branch called *colorways* to the remote repository. If you are on any major git platform with your project, it will come with a decent web GUI. Such a GUI is the most straight forward way to do the next step: get your Pull Request (PR) going.

<img src="images/pr.png" width="700px">
<div class="caption-left">Github pull request dialog: Select the *pull request*, choose which branch you merge into which target branch.</div>

As you can see, git will check whether it is possible to merge automatically w/o interaction. Even if that is not possible, you can still issue the pull request. When you create the request you can also assign reviewers, but you could also do so at a later stage. 

```{block, type='note'}

**Note**, even after a PR was issued you can continue to add commits to the branch about to be merged. 
As long as you do not merge the branch through the Pull Request, commits are added to the branch. In other 
words your existing PR gets updated. This is a very natural way to account for reviewer comments. 

```


```{block2, type='note'}
**Pro-Tipp**: 
Use commit messages like 'added join to SQL query, closes #3'. The key word 'closes' or 'fixes', will automatically close issues referred to when merged into the main branch.

```

Once the merge is done, all your changes are in the main branch and you and everyone else can pull the main branch that now contains your new feature. Yay!


## Project Management Basics

The art of stress free productivity as I once called it in '10 blog post, has put a number of gurus on the map and whole strand of literature to our bookshelves. So rather than adding to that, I would like to extract a healthy, best-of-breed type of dose here. The following few paragraphs do not intend to be comprehensive -- not even for the scope of software projects, but inspirational. 

In tje software development startup community, the *waterfall* approach became synonym to conservative, traditional and ancient: Overspecification in advance of the project, premature optimization and a lawsuit over expectations that weren't met. Though waterfall project may be better than their reputation and specifications should not be too detailed and rigid.

Many software projects are rather organized in *agile* fashion with SCRUM and KANBAN being the most popular derivatives. 
Because empirical academic projects have a lot in common with software projects inasmuch that there is a certain expectation and quality control, but the outcome is not known in advance. Essentially in agile project management you roughly define an outcome along the lines of a minimum viable product (MVP). That way you do not end up with nothing after a year of back and forth. 
During the implementation you'd meet regularly, let's say every 10 days, to discuss development since the last meet and what short term plans for the next steps ahead. The team picks splits work into task items on the issue tracker and assigns them.  Solution to problems will only be sketched out and discussed bilaterally or in small groups. By defining the work package for only a short timespan, the team stays flexible. In professional setups agile development is often strictly implemented and makes use of sophisticated systems of roles that developers and project managers can get certified for. 

Major git platforms ship with a decent, carpentry level project management project management GUI. The issue tracker is at the core of this. If you use it the minimal way, it's simply a colorful to-do list. Yet, with a bit of inspiration and use of tags, comments and projects, an issue tracker can be a lot more

<img src="images/issue.png">
<div class="caption-left">The Github issue tracker (example from one of the course's repositories) can be a lot more than a todo list.</div>

Swimlanes (reminiscant of a bird's eye view of an Olympic pool) can be thought of columns that you have to walk through
from left to right: To Do, Doing, Under Review, Done. (you can also customize the number and label of lanes and event associate actions with them, but let's stick to those basic lanes in this section.) The idea is to use to keep track of the process and making the process transparent. 


<img src="images/swim.png">
<div class="caption-left">GitHub's web platform offers swimlanes to keep a better overview of issues being worked on.</div>

```{block3, type='note'}
**Tipp**: 
No lane except 'Done' should contain more than 5-6 issues. 
Doing so prevents clogging the lanes at particular stage which 
could potentially lead to negligent behavior, e.g., careless reviews. 

```





## Testing

When talking about development practices, testing can't be missing. So, just you know that I thought of this, tbc ...














# Appendix {-}

## Glossary {-}

```{r, eval=TRUE,message=FALSE, echo=FALSE}
library(kableExtra)
d <- data.frame("Term" = c(
  "API",
  "Deployment",
  "Fork",
  "GUI",
  "IDE",
  "Merge Request",
  "OS",
  "OSS",
  "Pull Request (PR)",
  "Regular Expression",
  "Reproducible Example",
  "Stack",
  "SQL",
  "Swimlanes",
  "Virtual Machine (VM)"
  ),
  "Description" = c(
    "**A**pplication **P**rogramming **I**nterface",
    "The art of delivering a piece software to production",
    "a clone of a repository that you (usually) do not own.",
    "**G**raphical **U**ser **I**nterface",
    "**I**ntegrated **D**evelopment **E**nvironment",
    "See Pull Request.",
    "**O**perating **S**ysten",
    "**O**pen **S**ource **S**oftware",
    "Request to join a feature branch into another branch, e.g., main branch. Sometimes it's also called merge request.",
    "Pattern to extract specific parts from a text, find stuff in a text.",
    "A self-contained code example, including the data it needs to run.",
    "selection of software used in a project",
    "**S**tructured **Q**uery **L**anguage",
    "(Online) Board of columns (lanes). Lanes progress from from left to right and carry issues.",
    "A virtual computer hosted on your computer. Often used to run another OS inside your main OS for testing purposes."
    
  ),
  stringsAsFactors = FALSE)
kable(d,"html",escape= FALSE)

```


<!-- ## Testing --> 



















