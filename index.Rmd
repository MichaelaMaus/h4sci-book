--- 
title: "Hacking for Social Scientists"
subtitle: "A Guide to Programming With Data"
author: "Matthias Bannert"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
css: ["h4sci.css"]
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "The vast majority of data has been created within the last decade. In turn many fields of research are confronted with an unprecented wealth of data. The sheer amount of information but also the complexity of modern datasets continues to point researchers to programming approaches who had not considered programming to process data so far. Hacking for Social Sciences and Humanities"
---

> "There two things users hate,
> *change*, and the way things are."
> `r tufte::quote_footer('--- \\@TheWierdWorld (ShowerThoughts Twitter Account)')`

# Preface

The vast majority of data has been created within the last decade. In turn many fields of research are confronted with an unprecented wealth of data. The sheer amount of information but also the complexity of modern datasets continues to point a kind researcher to programming approaches who had not considered programming to process data so far. Hacking for Social Sciences and Humanities aims to give a big picture overview and starting point to reach what the open source software community calls a 'software carpentry' level. Also, this book argues a solid software carpentry skill level is totally in reach for most researchers. And most importantly, investing is worth the effort: being able to code leverages field specific expertise and fosters interdisciplinary collaboration as source code continues to become an important communication channel. 

# Introduction - The Choice that Doesn't Matter

The very first (and intimidating) choice a novice hacker faces is which is programming language to learn. Unfortunately the medium popularily summed up as the internet offers a lot of really really good advice on the matter. The problem is, however, that this advice does not necessarily agree which language is the best for research. In the realm of data science -- get accustomed to that label if you are a scientist who works with data -- the debate basically comes down to two languages: The R Language for Statistical Computing and Python. 

At least to me, there is only one valid advice: **It simply does NOT matter**. If you stick around in data science long enough you will eventually get in touch with both languages and in turn learn both. There is a huge overlap of what you can with both languages. R came out of the rather specific domain of statiscs 25+ years ago and made its way to a more general programming language thanks to 15K+ extension packages (and counting). Built by a mathmatician Python continues to be as general purpose as ever but got more scientific thanks to packages such as {pandas}, {sciPy} or {numPy}. As a result there is a huge overlap of what both languages can do and both will extend your horizon in unprecendented fashion if you did not use a full fledged programming language for your analysis before. 

But why is there such a heartfelt debate online, if it doesn't matter? Let's pick up a random argument from this debate: R is to set up and Python is better for machine learning. If you worked with Java or another environment that's rather tricky to get going, you are hardened and might not need or cherish easy onboarding. If you got frustrated before you even really started trying to reproduce a nifty machine learning blog post because you installed the wrong version of Python or didn't manage to make sense of virtualenv you might feel otherwise. 

The point is,  rest assured, if you just start doing analytics using a programming languages both languages are guaranteed to carry you a long way. There is no way to tell for sure which one will be the more dominant language in 10 years from now or whether both still be around holding their ground the way they do now. But once you reached a decent software carpentry level in either language it will help you a lot learning the other. If your peers work with R, start with R, if your close community
works with Python, start with Python. If you are in for the longer run either language will help you understand the concepts and ideas of programming with data. Trust me, there will be a natural opportunity to get to know the other. 


## Why Would Social Scientists Want to Code?

First of all, because everybody and their grandmothers seem to do it. Statistical computing continues to be on the rise in many branches of social sciences. 



<!-- The below graph shows monthly accumulated extension package downloads for the R Language of Statistical Computing grouped by fields of research^[proxied this and that].
Econometrics, Psychometrics, National Language Processing, Official Statistics, Social Sciences

```{r, eval=TRUE}
# add CRAN TASK VIEW Download 
# Statistics graph here
rnorm(10)
```

-->

Second because it's reproducible. Code has become a tremendous communication channel. Your web scraper does not work? 
Instead of reaching out in a clumsy but wordy cry for help, posting what you tried to far described by source code will often get you good answers within hours on platforms like [Stackoverflow](https://stackoverflow.com) or [Crossvalidated](https://crossvalidated.com). Or imagine feature requests, after a little code ping pong with the package author your wish eventually becomes clearer. Let alone chats with colleagues and co-authors. Sharing code just works. 
Academic journals have found out, too in the meantime. Many outlets require you to make the data and source code behind your work available. [Social Science Data Editors](https://social-science-data-editors.github.io/guidance/template-README.html) is a bleeding edge project at the time of reading this, but is already referred to by top notch journals of the profession like American Economic Review (AER). 

Third, because it scales and automates. Automation is not only convenient. Like when you want to download data, process and create the same visualization and put it on your website any given Sunday. Automation is inevitable. Like when you have to gather daily updates from different outlets or work through thousands of .pdfs. 

Last but not least because of things you couldn't do w/o being an absolute guru (if at all) if wasn't for programming. Take  visualization. Go, check [these D3 Examples](https://d3js.org/). Now, try to do that in Excel. If you do these things in Excel it'd make you an absolute spreadsheet visualization Jedi, probably missing out on other time consuming skills to master. Moral of the story is, with decent, carpentry level programming skills -- that'd be the upfront investment -- you can already do so many spectular things while not really specializing and staying very flexible. 




## How to Read this Book? 

*Hacking for Social Sciences* is written based on the experience of helping students and seasoned researchers of different fields with their data management, processing and communication of results. A part of the book contains the information I wish I had when I started a PhD in economics. Part of the book is written years after said PhD was completed and with the hindsight of 10+ years in academia. Every page of the book is written with the belief that the future is OPEN and it is up to our generation of researchers to shape it. 

<!-- add ad from Germany "the future is OPEN" for open data and open source 
https://www.bildung-forschung.digital/files/191004_OA-Infoflyer_barrierefrei.pdf
-->

>If you came to ```r emo::ji("cherry")``` pick, you're welcome, too (but a bit early to the party though). This book will grow along the 2020 course 'Hacking for Social Science' and hopefully be finished in its first version by the end of the semester / year. Next up are chapters on the *Big Picture of Open Source Software for hacking data* and *Git version control*. 


# Stack - A Developer's Toolkit

Just like natural craftsmen, digital carpenters depend on their toolbox and their mastery of it. 
*Stack* is what developers call the choice of tools used in a particular project. Even though different flavors come down to personal preferences, there is a lot of common ground in *programming with data* stacks. Throughout this book, often a choice for one piece of software needs to be made in order to illustrate things. But please do understand these choices as examples and focus on the role of an item in the bigger picture. To help you with the big picture of which tool does what the following section will group common programming-with-data stack components. Also, note notice that not every role has to be filled in every project. 

<img src="images/dr_egghead_panics.jpg" width="700px">
<div class="caption">Aaaaaaah! Dr. Egghead is overwhelmed by the plethora of tools.</div>

Don't panic, Dr. Egghead! All these components are here to help you and there is plenty of choices...
Here are the components I use most often. Note, this is a personal choice which works for me. Obviously not ALL of these are components are used in every small project. *Git, R* and *R Studio* would be the very minimal version, I guess. 

```{r, eval=TRUE,message=FALSE, echo=FALSE}
library(kableExtra)
d <- data.frame(Component = c("Interpreter / Language", "IDE / Editor",
                              "Version Control","Project Management",
                              "Database", "'Virtual' Environments", "Website Hosting",
                              "Workflow Automation", "Continous Integration"),
                Choice = c("<a href='https://r-project.org'>R</a>, <a href='https://www.python.org/'>Python</a>, <a href=''>Javascript</a>","<a href='https://rstudio.com'>R Studio</a>,<a href='https://code.visualstudio.com/'>VS Code</a>, <a href='https://www.sublimetext.com/'>Sublime</a>",
                           "<a href='https://git-scm.com/'>Git</a>", "<a href='https://github.com/features/project-management/'>GitHub</a>, <a href='https://about.gitlab.com/solutions/project-management/'>GitLab</a>","<a href='https://www.postgresql.org/'>PostgreSQL</a>","<a href='https://www.docker.com/'>Docker</a>",
                           "<a href='https://netlify.com/'>Netlify</a>, <a href='https://pages.github.com/'>GitHub Pages</a>","<a href='https://airflow.apache.org/'>Apache Airflow</a>", "<a href='https://docs.gitlab.com/ee/ci/'>GitLab CI</a>"),
                stringsAsFactors = FALSE)
kable(d,"html",escape= FALSE)

```


## Languages: Compiled vs. Interpreted

In Statistical Computing -- at least in Social Sciences -- the interface between the researcher and the computation node is almost always an interpreted progamming language as opposed to a compiled one. Compiled languages like C++ require the developer to write source code and compile, i.e. translation of the source code into what a machine can work with happens *before* runtime. The result of the compilation process is a binary which is specific to the operating system. Hence you will need a version for Windows, one for OSX and one for Linux if you intend to reach a truly broad audience with your program. 
The main advantage of a compiled language is speed in terms of computing performance as the translation into machine language does not happen during runtime. A reduction of development speed and increase in required developer skills are the downside of using compiled languages. 

> Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it. </br></br>
-- Dan Ariely, Professor of Psychology and Behavioral Economics, <a href="https://twitter.com/danariely/status/287952257926971392">on twitter</a>

The above quote became famous in the hacking data community, not only because of the provocative, fun part of it, but also because of the implicit advice behind it. Given the enormous gain in computing power in recent decades, but also methodological advances, interpreted languages are often fast enough for many social science problems. And even if it turns out, your data grow out of your setup, a well written proof of concept written in an interpreted language can be a formidable blueprint. **Source code is turning into an important scientific communication channel.** Put your money on it, your interdisciplinary collaborator from the High Performance Computing (HPC) group, will prefer some Python code as a briefing for their C++ or FORTRAN program over a wordy description out of your field's ivory tower. 

Interpreted language are a bit like pocket calculators, you can look at intermediate results, line by line. 
R and Python are the most popular OSS choices in hacking with data, <a href="https://julialang.org/">Julia</a> is an up and coming, perfomance focused language with a much slimmer ecosystem. A bit of Javascript can't hurt for advanced customization of graphics and online communication of your results.


<!-- Julia screenshot with caption -->


## IDE

It's certainly possible to move a four person family into a new home by public transport, but it is not convenient. The same holds for (plain) text editors in programming. You can use them, but most people would prefer an Integrated Development Environment (IDE) just like they prefer to use a truck when they move. IDEs are tailored to the needs and idiosyncrasies of a language, some working with plugins and covering multiple languages. Others have a specific focus on a single language or a group of languages. Here are some of the features you are looking for in an IDE for programming with data

- [code highlighting](https://en.wikipedia.org/wiki/Syntax_highlighting), linting
- decent file explorer
- terminal integration
- git integration
- markdown support
- debugging tools
- build tools
- customizable through add ins / macros


For R, the Open Source Edition of [R Studio Desktop](https://rstudio.com/products/rstudio/) is the right choice for most people.
(If you are working in a team, R Studio's server version is great. It allows to have a centrally managed server which clients can use through their a web browser without even installing R and R Studio locally.) R Studio has solid support for a few other languages often used together with R, plus it's customizable. The French premier thinkR [Colin_Fay]() gave a nice tutorial on [Hacking R Studio](https://speakerdeck.com/colinfay/hacking-rstudio-advanced-use-of-your-favorite-ide) at the useR! 2019 conference.

<img src="images/rstudio_r_py.png" width="350px">
<div class="caption-half">Screenshot of the coverpage of the R Studio <a href="https://rstudio.com">website</a> in fall 2020. The site advertises R Studio as an IDE for both languages R and Python. Remember <a href="">The Choice that Doesn't Matter?</a></div>

While R Studio managed to hold its ground among R aficionados as of fall 2020, Microsoft's free *Visual Studio Code* has blown the competition out of the water otherwise. Microsoft's IDE is blazing fast, extendable and polyglot. [VS Code Live Share](https://visualstudio.microsoft.com/services/live-share/) is just one rather random example of its remarkably well implemented features. Live share allows developers to edit a document simultaneously using multiple cursors in similar fashion to Google Docs, but with all the IDE magic and in a Desktop client. 

Another approach is to go for a highly customizable editor such as *Sublime* or *Atom*. 
The idea is to send source code from the editor to interchangeable REPLs (read-eval-print-loops)
which can be adapted according to the language that needs to be interpreted. That way a good linter / code highlighter for your favorite language is needed for the editor and you have a lightweight environment to run things. An example of such a customization approach is Christoph Sax OSS small project [Sublime Studio](https://github.com/christophsax/SublimeStudio). 

Other examples for popular IDEs are Eclipse (mostly Java but tons of plugins for other languages), IntelliJ (Java) and PyCharm (Python). 


## Version Control: Git

To buy into the importance of managing one's code professionally may be the single most important take away from *Hacking for Social Sciences*. While version control has been the de facto standard in many industries for decades, it has just started to be everywhere in research in recent years. It's the one basic thing you learn during your time at university that graduates before you may not have learned.  

So which role does version control play for us? It keeps track of the evolution of our code 








## Database: Relational vs. Non-Relational 

To evaluate which database to pick up just seems like the next daunting task of stack choice. Luckily, in research first encounters with a database are usually passive, in the sense that you want to query data from a source that uses a particular database. So unless you want to start your own data collection from, simply sit back, relax and let the internet battle out another conceptual war. 

Database Management Systems (DBMS) are basically grouped into [relational]() and [non-relational]() ones. Relational databases with their Structured Query Language (SQL) have been around forever. SQL became and ISO and ANSI standard and continue to be essence of many many backends around the world. Oracle continues to be the benchmark for SQL databases but opensource PostgreSQL and Microsoft's SQL Server operate at eye level for many applications.
MySQL, Oracle's slim, little (but free) brother, can't quite cope with PostgreSQL, continues to be the most used SQL database on the planet. This is mainly due to its popularity for web applications like the blogging [CMS]() [wordpress](). Last but not least, *sqlite* needs to be mentioned when talking about relational database. The name nutshells its concept quite well: It's an easy to use, much simpler version of the aforementioned database management systems. It is extremely popular for light but powerful applications that want to organize data with a SQL approach in a single file (mobile applications like to use it for example).

No-SQL databases are the anti establishment, anti standard approach. [MongoDB]() may be the best marketed among the rebels. Before you start to sympathize with latter approach because the wording of my last to sentences, let's stop here. Database experts like [Lukas Eder]() maybe biased but much better educated to educate you here. The idea of this chapter is just to help you group all the database stores you might face to soon as a researchers. 

The good news is, languages like R and Python are so well equipped to interface with a plethora of databases. So well that I often recommend these languages to researcher who work with other less equppied tools, solely as an intermediate layer. And if there is really no database extension around for your language, a general [ODBC]() interface helps -- at least for SQL databases.  




## Single Purpose Environments: Docker

Side Effect free working environment.


## Communication

Communication is an essentially part of building an (academic) career. Part of it is a neat online profile. Do not relax on the excuse that are department's website does not offer the flexibility. The legal and technical situation in many places should allow you to spin up your own website or even run a blog if you find the time. For free. Including the web hosting. 

A popular approach to do so is to work with a *static website generator*. Generators like blogdown, pkgdown or bookdown are flavors of the same approach to create a website from markdown first and then upload rendered HTML + CSS + Javascript page to a host like GitHub Pages or Netlify that allow you to store it online without paying. 

The idea of engines like [Hugo]() or [Jekyll]() which are behind the above packages is a counter approach to what content management systems do [CMS](): There is no database and templates that are brought together dynamically when a user visits the website. This rendering is done locally on the creator's local notebook. Whenever a change is made, the website is rendered entirely (ok, minus caching) and uploaded (pushed) again to the host. Therefore no database, etc is needed on the webserver which cuts down the hosting costs to zero. (FWIW: this book is made with such a generator) 




## Workflow Automation






# Programmers' Practices & Workflows

Just like most experienced engineers, seasoned software developers follow some kind of school or paradigm. Good programmers can even switch among approaches according to their current project's needs or depending on the team they are on. 

This section does not want to give a comprehensive overview over programming concepts nor compare approaches. And damn sure it does not mean to go to war over approach superiority. *Hacking for Social Scientists* rather cherry-picks suitable application-minded, low-barrier concepts that help social scientists professionalize their own programming. 


## Version Control

### What is Git Version Control? 

Git is *not* Dropbox nor Google Drive. It does *not* sync automagically even if some Git GUI Tools suggest so. As opposed to that, a version control system allows to summarize a contribution across files and folders based on what this contribution is about. Assume you got a cool pointer from an econometrics professor at a conference and you incorporated her advice in your work. That advice is likely to affect different parts of your work: your text and your code. As opposed to syncing each of these files based on the time you saved them, version control creates a version when you decide to bundle things together and to commit the change. 

### Why Use Version Control in Research? 

>A version control based workflow fosters a path to your goals that rather consists of semantically relevant steps instead of random chunks based on time. 

Just in case the above paragraph about what version control is not enough of a justification to use git, let me be more blatant here: Naming your files like 
`final_version_your_name.R` or `final_final_correction_collaboratorX_20200114.R` 
is like given your WiFi a funny to communicate with your neighbors. Like `dont_park_the_car_in_the_frontyard` or `be_quiet_at_night`. This information is supposed to be sent in a message, not a file name. With version control it is immediately clear what the most current version is - no matter the file name. No room for interpretation. No need to start guessing about the delta between current or final and most recent. 

Also, you can easily try out different scenarios on different branches and merge them back together if you need to. Version control is a well established industry standard in software development. And it is relatively easy to adopt. With datasets growing in size and complexity in social sciences as well it is only natural to improve management of the code that processes these data. 



>No offense, if you are new to version control, it means you are new to programming.

Academia is probably the only place would allow you to dive into hacking at somewhat complex problems for several years w/o ever taking notice of version control. As a social scientist who rather collaborates in small groups and writes moderate amount of code, have you ever thought about how to collaborate with 100+ persons big software project? Or you to manage ten thousands of lines of code and beyond? Version control is just one of the reasons these things work. And it's been around for decade. Today, the decentralized version control system *git* has become the de facto standard of the open source community to manage code. 

### Git Basics

Git is a decentralized version control system which means all versions are stored on the local machines of every collaborator, not just on a remote server. This allows for a great workflow when it comes to putting versions together. 

But let's start at the very beginning. The git programm itself is a small console programm that is typically used through the console. You can install graphical user interfaces (GUI) on top of git but doing so does not really improve your understanding of the workflow. That's why the following examples straight up use the console -- even if you might choose a GUI tool like *Tortoise*, *SourceTree* or *Github Desktop* in the end. Also, many IDEs have git integration or offer plugins at least. 


```{r, eval=TRUE}
# INSERT PICTURE OF A BASH HERE
rnorm(10)
```

On the other hand you can navigate projects pretty well with about two hands full of git commands. Let us walk through the most important basic commands. 


- git clone: clone an entire repository from remote to your local disk. This keeps the address of the remote repository it's been cloned from. Use github.com's or gitlab.com's web site to create a repository. By cloning we do not only create a local copy but also set up the remote url. Nifty.

- git init: If you don't think about remote repositories and github.com accounts immediately, 
you can make any folder a git repository, i.e., put it under version control. *Beware*: Make sure not to initialize a repository in a repository!

- git add: add files to the staging area. The staging areas determines which changes will be committed when the next commit is triggered

- git commit -m 'some meaningful message': Create a new version, consisting of the changes made to the files in the staging area.

- git push: move all new commits to the remote repository (server). 

- git pull: get all new commits to the remote repository (server). 

- git checkout: Switch to another version / branch.

- git log: Show list of commit messages and commit identifiers. 

- git branch: Show available branches / current branch.


### Feature Branches - How to Use Git Efficiently in Teams

A very common and intuitive way to colloborate using git is to use feature branches:  












https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow
Eric Sink Version Control by Example
https://happygitwithr.com/


## Project Management 

## How to Document

## Development (Thinking Packages)

## Testing

Unit tests vs. regression tests.



## Deployment - Continous Integration

## Operations

## Collaboration



# Organize & Manage Data

## file based vs db

## SQL vs NoSQL



# Programming

## General Advice

## Scraping an Online Resource - Case Study

## Making the most of APIs - Case Study 

## Setup a Non-Interactive Batch Process /w tryCatch - Case Study 

## Cloud Computing - Case Study 

googleCloudStorageR
https://cloud.google.com/compute/docs/apis
https://cloudyr.github.io/googleComputeEngineR/

https://kubernetes.io/blog/2017/08/kubernetes-meets-high-performance/
https://www.hpcwire.com/2019/09/19/kubernetes-containers-and-hpc/

- containers on compute engine don't need kubernetes
https://cloud.google.com/compute/docs/containers/

- question for the google dude: Kubernetes for HPC
- *jobs that run to completion* 
- cost control? Slurm like ? you does it work in an organization, when everybody 
fires up their GCEs? 
- is there scheduler or do I have to fire up a scheduler VM
- https://github.com/kube-HPC/hkube
- https://en.paradigmadigital.com/dev/apache-airflow/
https://cloud.google.com/python/

## Online and Print - Case Study 

## Reproducible Reporting with RMarkdown and Knitr - Case Study 

## Using Docker as Sandbox - Case Study







