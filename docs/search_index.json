[
["index.html", "Hacking for Social Scientists A Guide to Programming With Data 1 Preface", " Hacking for Social Scientists A Guide to Programming With Data Matthias Bannert 2020-09-26 1 Preface The vast majority of data has been created within the last decade. In turn many fields of research are confronted with an unprecented wealth of data. The sheer amount of information but also the complexity of modern datasets continues to point a kind researcher to programming approaches who had not considered programming to process data so far. Hacking for Social Sciences aims to give a big picture overview and starting point to reach what the open source software community calls a ‚Äòsoftware carpentry‚Äô level. Also, this book argues a solid software carpentry skill level is totally in reach for most researchers. And most importantly, investing is worth the effort: being able to code leverages field specific expertise and fosters interdisciplinary collaboration as source code continues to become an important communication channel. Meet Dr. Egghead who started his quest to figure out how his assistant got a week‚Äôs work done in two hours. ‚ÄúHacker‚Äôs wear hoodies, you know‚Äù, &quot; he mumbles as he starts to think1 at useR! 2019. FWIW, Dr. Egghead was born after Colin Gillespie‚Äôs hilarious talk on Security in Academia‚Ä¶‚Ü© "],
["introduction-the-choice-that-doesnt-matter.html", "2 Introduction - The Choice that Doesn‚Äôt Matter 2.1 Why Would Social Scientists Want to Code? 2.2 How to Read this Book?", " 2 Introduction - The Choice that Doesn‚Äôt Matter The very first (and intimidating) choice a novice hacker faces is which is programming language to learn. Unfortunately the medium popularily summed up as the internet offers a lot of really really good advice on the matter. The problem is, however, that this advice does not necessarily agree which language is the best for research. In the realm of data science ‚Äì get accustomed to that label if you are a scientist who works with data ‚Äì the debate basically comes down to two languages: The R Language for Statistical Computing and Python. At least to me, there is only one valid advice: It simply does NOT matter. If you stick around in data science long enough you will eventually get in touch with both languages and in turn learn both. There is a huge overlap of what you can do either of those languages. R came out of the rather specific domain of statiscs 25+ years ago and made its way to a more general programming language thanks to 15K+ extension packages (and counting). Built by a mathmatician, Python continues to be as general purpose as it‚Äôs ever been. But it got more scientific, thanks to extension packages of its own such as pandas, SciPy or numPy. As a result there is a huge overlap of what both languages can do and both will extend your horizon in unprecendented fashion if you did not use a full fledged programming language for your analysis before. R: ‚ÄúDplyr smokes pandas.‚Äù Python: ‚ÄúBut Keras is better for ML!‚Äù Language wars can be entertaining, sometimes spectacular, but most times they are just useless‚Ä¶ But why is there such a heartfelt debate online, if it doesn‚Äôt matter? Let‚Äôs pick up a random argument from this debate: R is easier to set up and Python is better for machine learning. If you worked with Java or another environment that‚Äôs rather tricky to get going, you are hardened and might not cherish easy onboarding. If you got frustrated before you really started, you might feel otherwise. You may just have been unlucky making guesses about a not so well documented paragraph, trying to reproduce a nifty machine learning blog post. Just because you installed the wrong version of Python or didn‚Äôt manage to make sense of virtualenv right from the beginning. The point is, rest assured, if you just start doing analytics using a programming languages both languages are guaranteed to carry you a long way. There is no way to tell for sure which one will be the more dominant language in 10 years from now or whether both still be around holding their ground the way they do now. But once you reached a decent software carpentry level in either language, it will help you a lot learning the other. If your peers work with R, start with R, if your close community works with Python, start with Python. If you are in for the longer run either language will help you understand the concepts and ideas of programming with data. Trust me, there will be a natural opportunity to get to know the other. 2.1 Why Would Social Scientists Want to Code? First of all, because everybody and their grandmothers seem to do it. Statistical computing continues to be on the rise in many branches of social sciences. Second because it‚Äôs reproducible. Code has become a tremendous communication channel. Your web scraper does not work? Instead of reaching out in a clumsy but wordy cry for help, posting what you tried so far described by source code will often get you good answers within hours on platforms like Stackoverflow or Crossvalidated. Or imagine feature requests, after a little code ping pong with the package author your wish eventually becomes clearer. Let alone chats with colleagues and co-authors. Sharing code just works. Academic journals have found that out, too in the meantime. Many outlets require you to make the data and source code behind your work available. Social Science Data Editors is a bleeding edge project at the time of writing this, but is already referred to by top notch journals of the profession like American Economic Review (AER). Third, because it scales and automates. Automation is not only convenient. Like when you want to download data, process and create the same visualization and put it on your website any given Sunday. Automation is inevitable. Like when you have to gather daily updates from different outlets or work through thousands of .pdfs. Last but not least because of things you couldn‚Äôt do w/o being an absolute guru (if at all) if wasn‚Äôt for programming. Take visualization. Go, check these D3 Examples. Now, try to do that in Excel. If you do these things in Excel it‚Äôd make you an absolute spreadsheet visualization Jedi, probably missing out on other time consuming skills to master. Moral of the story is, with decent, carpentry level programming skills ‚Äì that‚Äôd be the upfront investment ‚Äì you can already do so many spectular things while not really specializing and staying very flexible. 2.2 How to Read this Book? Hacking for Social Sciences is written based on the experience of helping students and seasoned researchers of different fields with their data management, processing and communication of results. A part of the book contains the information I wish I had when I started a PhD in economics. Part of the book is written years after said PhD was completed and with the hindsight of 10+ years in academia. Every page of the book is written with the belief that the future is OPEN and it is up to our generation of researchers to shape it. ‚ÄúThe ministry warns: The future is open,‚Äù taken from a 2020 ad campaign on Open Access by the German ministry for education and research (pdf). If you came to üçí pick, you‚Äôre welcome, too (but a bit early to the party though). This book will grow along the 2020 course ‚ÄòHacking for Social Science‚Äô and hopefully be finished in its first version by the end of the semester / year. Next up are chapters on the Big Picture of Open Source Software for hacking data and Git version control. "],
["stack-a-developers-toolkit.html", "3 Stack - A Developer‚Äôs Toolkit 3.1 Languages: Compiled vs. Interpreted 3.2 IDE 3.3 Version Control 3.4 Database: Relational vs. Non-Relational 3.5 Single Purpose Environments 3.6 Communication 3.7 Automation", " 3 Stack - A Developer‚Äôs Toolkit Just like natural craftsmen, digital carpenters depend on their toolbox and their mastery of it. Stack is what developers call the choice of tools used in a particular project. Even though different flavors come down to personal preferences, there is a lot of common ground in programming with data stacks. Throughout this book, often a choice for one piece of software needs to be made in order to illustrate things. But please notice that these choices are examples and focus on the role of an item in the big picture. To help you with the big picture of which tool does what, the following section will group common programming-with-data stack components. Also, notice that not every role has to be filled in every project. Aaaaaaah! Don‚Äôt panic, Dr. Egghead! All these components are here to help you and you won‚Äôt need all of them every from the start. Here are some components I use most often. This is a personal choice which works for me. Obviously not ALL of these are components are used in every small project. Git, R and R Studio would be my very minimal version. Component Choice Interpreter / Language R, Python, Javascript IDE / Editor R Studio,VS Code, Sublime Version Control Git Project Management GitHub, GitLab Database PostgreSQL ‚ÄòVirtual‚Äô Environments Docker Communication (Visualization, Web) Node, Quasar (vue.js) Website Hosting Netlify, GitHub Pages Workflow Automation Apache Airflow Continous Integration GitLab CI 3.1 Languages: Compiled vs. Interpreted In Statistical Computing ‚Äì at least in Social Sciences ‚Äì the interface between the researcher and the computation node is almost always an interpreted progamming language as opposed to a compiled one. Compiled languages like C++ require the developer to write source code and compile, i.e., translate source code into what a machine can work before runtime. The result of the compilation process is a binary which is specific to the operating system. Hence you will need one version for Windows, one for OSX and one for Linux if you intend to reach a truly broad audience with your program. The main advantage of a compiled language is speed in terms of computing performance because the translation into machine language does not happen during runtime. A reduction of development speed and increase in required developer skills are the downside of using compiled languages. Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it. ‚Äì Dan Ariely, Professor of Psychology and Behavioral Economics, on twitter The above quote became famous in the hacking data community, not only because of the provocative, fun part of it, but also because of the implicit advice behind it. Given the enormous gain in computing power in recent decades, but also methodological advances, interpreted languages are often fast enough for many social science problems. And even if it turns out, your data grow out of your setup, a well written proof of concept written in an interpreted language can be a formidable blueprint. Source code is turning into an important scientific communication channel. Put your money on it, your interdisciplinary collaborator from the High Performance Computing (HPC) group, will prefer some Python code as a briefing for their C++ or FORTRAN program over a wordy description out of your field‚Äôs ivory tower. Interpreted languages are a bit like pocket calculators, you can look at intermediate results, line by line. R and Python are the most popular OSS choices in hacking with data, Julia is an up and coming, perfomance focused language with a much slimmer ecosystem. A bit of Javascript can‚Äôt hurt for advanced customization of graphics and online communication of your results. 3.2 IDE It‚Äôs certainly possible to move a five person family into a new home by public transport, but it is not convenient. The same holds for (plain) text editors in programming. You can use them, but most people would prefer an Integrated Development Environment (IDE) just like they prefer to use a truck when they move. IDEs are tailored to the needs and idiosyncrasies of a language, some working with plugins and covering multiple languages. Others have a specific focus on a single language or a group of languages. Here are some of the features you are looking for in an IDE for programming with data: code highlighting, linting decent file explorer terminal integration git integration markdown support debugging tools build tools customizable through add-ins / macros For R, the Open Source Edition of R Studio Desktop is the right choice for most people. (If you are working in a team, R Studio‚Äôs server version is great. It allows to have a centrally managed server which clients can use through their a web browser without even installing R and R Studio locally.) R Studio has solid support for a few other languages often used together with R, plus it‚Äôs customizable. The French premier thinkR Colin_Fay gave a nice tutorial on Hacking R Studio at the useR! 2019 conference. Screenshot of the coverpage of the R Studio website in fall 2020. The site advertises R Studio as an IDE for both languages R and Python. Remember The Choice that Doesn‚Äôt Matter? While R Studio managed to hold its ground among R aficionados as of fall 2020, Microsoft‚Äôs free Visual Studio Code has blown the competition out of the water otherwise. Microsoft‚Äôs IDE is blazing fast, extendable and polyglot. VS Code Live Share is just one rather random example of its remarkably well implemented features. Live share allows developers to edit a document simultaneously using multiple cursors in similar fashion to Google Docs, but with all the IDE magic and in a Desktop client. Another approach is to go for a highly customizable editor such as Sublime or Atom. The idea is to send source code from the editor to interchangeable REPLs (read-eval-print-loops) which can be adapted according to the language that needs to be interpreted. That way a good linter / code highlighter for your favorite language is needed for the editor and you have a lightweight environment to run things. An example of such a customization approach is Christoph Sax OSS small project Sublime Studio. Other examples for popular IDEs are Eclipse (mostly Java but tons of plugins for other languages), IntelliJ (Java) and PyCharm (Python). 3.3 Version Control To buy into the importance of managing one‚Äôs code professionally may be the single most important take away from Hacking for Social Sciences. Being able to work with version control will help you fit into a lot of different teams that have contact points with data science and programming, let alone if you become part of a programming or data science team. While version control has a long history dating back to CVS and SVN, the good news for the learner is, that there is a single dominant approach when it comes to version control in academia. Despite the fact that it‚Äôs predecessors and alternative such as mercurial are stil around, git is the one you have to learn. To learn more about the history of version controls and approaches other than git, Eric Sink‚Äôs Version Control by Example is for you. So what does git do for us as researchers? How is it different from dropbox? git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. The idea of thinking of a sync, is what hampers from understanding the benefit of right away (which why I hate that git GUIs called it ‚Äòsync‚Äô anway). Git is a decentralized version control system that keeps track of a history of semantic commits that may consists of changes to multiple files. A commit message summarizes the gist of a contribution bundles a contribution. Diffs allow to changes between different versions. The diff output shows an edit during the writing of this book. The line preceeded by ‚Äò-‚Äô was replaced with the line preceeded by ‚Äò+‚Äô. Git is well suited for any kind of text file / source code from Python or C++ to markdown or LaTeX. Binaries like .pdfs or Word documents are possible, too, but certainly not the type of files for which git is really useful. This book contains a detailed, applied introduction tailed to researchers as part of the Programmers‚Äô Practices and Workflows chapter, so let‚Äôs dwell with the above contextualization for a bit. 3.4 Database: Relational vs. Non-Relational To evaluate which database to pick up just seems like the next daunting task of stack choice. Luckily, in research first encounters with a database are usually passive, in the sense that you want to query data from a source that uses a particular database. So unless you want to start your own data collection from, simply sit back, relax and let the internet battle out another conceptual war. Database Management Systems (DBMS) are basically grouped into relational and non-relational ones. Relational databases with their Structured Query Language (SQL) have been around forever. SQL became and ISO and ANSI standard and continue to be essence of many many backends around the world. Oracle continues to be the benchmark for SQL databases but opensource PostgreSQL and Microsoft‚Äôs SQL Server operate at eye level for many applications. MySQL, Oracle‚Äôs slim, little (but free) brother, can‚Äôt quite cope with PostgreSQL, continues to be the most used SQL database on the planet. This is mainly due to its popularity for web applications like the blogging CMS wordpress. Last but not least, sqlite needs to be mentioned when talking about relational database. The name nutshells its concept quite well: It‚Äôs an easy to use, much simpler version of the aforementioned database management systems. It is extremely popular for light but powerful applications that want to organize data with a SQL approach in a single file (mobile applications like to use it for example). No-SQL databases are the anti establishment, anti standard approach. MongoDB may be the best marketed among the rebels.Before you start to sympathize with latter approach because the wording of my last to sentences, let‚Äôs stop here. Large infrastructure players make the case for non-relational stores like CouchDB or Amazon Redshift Database, but trust me, you those are unlikely the first things you get to run when your research grows out of Excel. If your are not happy with the ‚Äòbeyond-the-scope-of-this-book‚Äô argument, blogging experts like Lukas Eder maybe biased but much better educated to educate you here. The idea of this chapter is just to help you group all the database stores you might face to soon as a researchers. The good news is, languages like R and Python are so well equipped to interface with a plethora of databases. So well that I often recommend these languages to researcher who work with other less equppied tools, solely as an intermediate layer. And if there is really no database extension around for your language, a general ODBC interface helps ‚Äì at least for SQL databases. 3.5 Single Purpose Environments Though a bit out of fashion and somewhat different, virtual machines are good starting point to explain single purpose container environments such as docker. A virtual machine (VM) is basically a computer in a computer, like a Linux environment running on your Windows notebook. Oracle‚Äôs free Virtual Box is the most popular piece of software to easily install another operating system inside your local computer‚Äôs host. While VMs are still common and one can potentially have lots of images for different purposes, images are too heavyweight and take to long to boot to be the go-to solution for many application developers. Hence so-called containers that run within a container host and fire up within seconds have become popular as single purpose environments. The most popular of them all is docker which allows users to configure an environment in a Dockerfile (essentially a text file) including the operating system and software packages installed in the container. The text file can either be used to create a docker image which is kind of a blueprint for a container. Containers run inside a docker host and can either be used interactively or in a batch which executes a single task in an environment specifically built for this task. One of the reasons why docker is attractive to researchers is its open character: Dockerfiles are a good way to share a configuration in a simple, reproducible script, making it easy to reproduce. Less experienced researchers can benefit from Dockerhub which shares images for a plethora of purposes from mixed data science setups to database configuration. Side Effect free working environments for all sorts of task can especially be appealing to developers with limited experience in system administration. Beside simplification of system administration, docker is known for its ability to work in the cloud. All major cloud hosters offer docker hosts and the ability to deploy docker containers that were previously developed and tested locally in the cloud. You can also use docker to tackle throughput problems using container orchestration tools like Docker Swarm or K8 (say: Kubernetes) to run hundreds of containers (depending on your virtual resources). 3.6 Communication Communication is an essentially part of building an (academic) career. Part of it is a neat online profile. Do not relax on the excuse that are department‚Äôs website does not offer the flexibility. The legal and technical situation in many places should allow you to spin up your own website or even run a blog if you find the time. For free. Including the web hosting. A popular approach to do so is to work with a static website generator. Generators like blogdown, pkgdown or bookdown are flavors of the same approach to create a website from markdown first and then upload rendered HTML + CSS + Javascript page to a host like GitHub Pages or Netlify that allow you to store it online without paying. The idea of engines like the Go based Hugo or the Ruby based Jekyll which are behind the above packages is a counter approach to what content management systems do CMS: There is no database and templates that are brought together dynamically when a user visits the website. This rendering is done locally on the creator‚Äôs local notebook. Whenever a change is made, the website is rendered entirely (ok, minus caching) and uploaded (pushed) again to the host. Therefore no database, etc is needed on the webserver which cuts down the hosting costs to zero. (FWIW: this book is made with such a generator) 3.7 Automation The first type of automation described here refers to automation of your development workflow. That it is, you standardize your path from draft to program to deployment of your program to production. Modern version control software accompanies this process with a toolchain that is often fuzzily called CI/CD. While CI stands for continuous integration and simply refers to a workflow in which the team tries to release new features to production as continuously as possible, CD stands for either continuous delivery or continuous deployment. However, in practice the entire toolchain referred to as CI/CD has become readily available in well documented fashion when git hosting powerhouses GitHub and GitLab introduced their flavors of it: [GitHub Actions])(https://docs.github.com/en/actions) and GitLab CI. In addition services like Travis CI or Circle CI offer this toolchain independently of hosting git repositories. Users of these platforms can upload a simple textfile that follows a name convention and structure to trigger a step based toolchain based on an event. An example of an event may be the push to a repository‚Äôs main branch. A common example would be to run tests and/or build process of a package and then upon success deploy the newly created package to some server ‚Äì all triggered by simple push to master. One particularly cool thing is, that there multiple services who allow to run the testing on their servers using container technologies to offer a plethora of setups for testing. This software can easily be tested on different operating systems or other dependencies. Here is a simple example of a .gitlab-ci.yml configuration that builds and tests a package and deploys it on push to master if the tests succeed: stages: - buildncheck - deploy_pack test: image: name: some.docker.registry.com/some-image:0.2.0 entrypoint: - &quot;&quot; stage: buildncheck artifacts: untracked: true script: - rm .gitlab-ci.yml # we don&#39;t need it and it causes a hidden file NOTE - install2.r --repos custom.mini.cran.ch . - R CMD build . --no-build-vignettes --no-manual - R CMD check --no-manual *.tar.gz deploy_pack: only: - master stage: deploy_pack image: name: byrnedo/alpine-curl entrypoint: [&quot;&quot;] dependencies: - &#39;test&#39; script: - do some more steps to login and deploy to server ... For more in depth examples of the above, Jim Hester‚Äôs talk on GitHub Actions for R is a very good starting point. The other automation too I would like to mention is Apache Airflow because of its ability to help researchers keep an overview of regularly running processes. Examples of such processes could be daily or monthly data sourcing or timely publication of a regularly published indicator. For processes we can simply use cronjobs to time execution, but Airflow ships with a dashboard to keep track of many such process, plus a ton of other log and reporting features worth a lot when maintaining reocurring processe processes. "],
["developer-practices-workflows.html", "4 Developer Practices &amp; Workflows 4.1 Git Version Control 101 4.2 Feature Branches, PRs and Forks 4.3 Project Management Basics 4.4 Testing", " 4 Developer Practices &amp; Workflows Just like most experienced engineers, seasoned software developers follow some kind of school or paradigm. Good programmers can even switch among approaches according to their current project‚Äôs needs or depending on the team they are on. This section does not want to give a comprehensive overview over programming concepts nor compare approaches. And damn sure it does not mean to go to war over approach superiority. Hacking for Social Scientists rather cherry-picks suitable application-minded, low-barrier concepts that help social scientists professionalize their own programming. 4.1 Git Version Control 101 Version control may be the single most important thing to take away from Hacking for Social Sciences. In this chapter about the way developers work, I will stick to version control with git. The stack discussion of the previous chapter features a few more version control systems, but given git‚Äôs dominant position, we will stick solely to git in this introduction to version control. 4.1.1 What is Git Version Control? Git is a decentralized version control system. It manages different versions of your source code (and other text files) in a simple put efficient manner that has become the industry standard: The git programm itself is a small console programm that creates and manages a hidden folder inside the folder you put under version control (you know those folders with a leading dot in their foldername, like .myfolder). This folder keeps track of all difference between the current version and other versions before the current one. Meaningful commit messages help to make sense of a project‚Äôs history. The key to appreciate the value of git is to appreciate the value of semantic versions. Git is not Dropbox nor Google Drive. It does not sync automagically (even if some Git GUI Tools suggest so). Because these GUIs tools2 may be convenient but do not really help to improve your understanding of the workflow, we will use the git console throughout this book. As opposed to the sync approaches mentioned above, a version control system allows to summarize a contribution across files and folders based on what this contribution is about. Assume you got a cool pointer from an econometrics professor at a conference and you incorporated her advice in your work. That advice is likely to affect different parts of your work: your text and your code. As opposed to syncing each of these files based on the time you saved them, version control creates a version when you decide to bundle things together and to commit the change. That version could be identified easily by its commit message ‚Äòincorporated advice from Anna (discussion at XYZ Conf 2020)‚Äô. 4.1.2 Why Use Version Control in Research? A version control based workflow is a path to your goals that rather consists of semantically relevant steps instead of semantically meaningless chunks based on the time you saved them. In other more blatant, applied words: naming files like final_version_your_name.R or final_final_correction_collaboratorX_20200114.R is like naming your WiFi dont_park_the_car_in_the_frontyard or be_quiet_at_night to communicate with your neighbors. Information is supposed to be sent in a message, not a file name. With version control it is immediately clear what the most current version is - no matter the file name. No room for interpretation. No need to start guessing about the delta between the current version and another version. Also, you can easily try out different scenarios on different branches and merge them back together if you need to. Version control is a well established industry standard in software development. And it is relatively easy to adopt. With datasets growing in size and complexity it is only natural to improve management of the code that processes these data. Academia is probably the only place would allow you to dive into hacking at somewhat complex problems for several years w/o ever taking notice of version control. As a social scientist who rather collaborates in small groups and writes moderate amount of code, have you ever thought about how to collaborate with 100+ persons big software project? Or you to manage ten thousands of lines of code and beyond? Version control is an important reason why these things work. And it‚Äôs been around for decades. But enough about the rant‚Ä¶ 4.1.3 How Does Git Work ? The first important implication of decentralized version control is that all versions are stored on the local machines of every collaborator, not just on a remote server. So let‚Äôs consider a single local machine first. Locally, a git repository consists of a checkout which is also called current working copy soon. This is the status of the file that your file explorer or your editor will see when you use them to open a file. To checkout a different version, one needs to find call a commit by its unique commit hash and checkout that particular version. If you want to add new files to version control or bundle changes to some existing files into a new commit, add these files to the staging area, so they get committed next time a commit process is triggered. Finally committing all these staged changes under another commit id a new version is created. 4.1.4 Moving Around So let‚Äôs actually do it. Here‚Äôs a three stage walk through of git commands that should have you covered in most use cases a researcher will face. Note that git has some pretty good error message that guess what could have gone wrong, make sure to read them carefully. Even you can‚Äôt make sense of them, your online search will be a lot more efficient when you include these messages. Stage 1: Working Locally Command Effect git init put current directory and all its subdirs under version control. git status shows status git add file_name.py adds file to tracked files git commit -m ‚Äòmeaningful msg‚Äô creates a new version/commit out of all staged files git log show log of all commit messages on a branch git checkout some-commit-id go to commit, but in detached HEAD state git checkout main-branch-name leave temporary state, go back to last commit Stage 2: Working with a Remote Repository Though git can be tremendously useful even without collaborators, the real fun starts when working together. The first step en route to get others involved is to add a remote repository. Command Effect git clone creates a new repo based on a remote one git pull get all changes from a linked remote repo git push deploy all commit changes to the remote repo git fetch fetch branches that were created on remote git remote -v show remote repo URL git remote set-url origin https://some-url.com set URL to remote repo Stage 3: Branches Command Effect git checkout -b branchname create new branch named branchname git branch show locally available branches git checkout branchname switch to branch named branchname git merge branchname merge branch named branchname into current branch Fixing Merge Conflicts In most cases git is quite clever and can figure out which is the desired state of a file when putting two versions of it together. When git‚Äôs recursive strategy is at work, it will just merge versions automatically. When the same lines were affected in different versions, git cannot tell which line should be kept. Sometimes you would even want to keep both changes. But even in such scenario fixing the conflict is easy. Git will tell you that your last command caused a merge conflict and which files are conflicted. Open these files and see all parts of the file that are in question. Ouch! We created a conflict by editing the same line in the same file on different branches and trying to but these branches back together. Luckily git marks the exact spot where the conflict happens. Good text editors / IDEs ship with cool colors to highlight all our options. go for the current status or take what‚Äôs coming in from the a2 branch? Some of the fancier editors even have git conflict resolve plugins that let you walk through all conflict points. In VS Code you can even click the option. At the and of the day, all do the same, i.e., remove the unwanted part including all the marker gibberish. After you have done so, save, commit and push (if you are working with a remote repo) . Don‚Äôt forget to make sure you kinked out ALL conflicts. 4.2 Feature Branches, PRs and Forks This section discusses real world collaboration workflows of modern open source software developers. Hence the prerequisites are a bit different in order to benefit the most from this section. Make sure you are past being able to describe and explain git basics, be able to create and handle your own repositories. If you had only a handful of close collaborators so far, you may be fine with staying on the main branch and trying not to step on each others feet. This is reasonable because it is not useful to work asynchronously on exact the same lines of code anyway. Nevertheless, there is a reason why feature-branch-based workflows became very popular among developers: Imagine you collaborate less synchronously, maybe with someone in another timezone. Or with a colleague who works on your project, but in a totally different month during the year. Or, most obviously, we someone you have never met. Forks and feature-branch-based workflows is the way a lot of modern open source projects are organized. Forks are just a way to contribute via feature branches even in case you do not have write access to a repository. But let‚Äôs just have look at the basic case in which you are part of the team first. Assume there is already some work done, some version of the project is already up on GitHub. You join as a collaborator and are allowed to push changes now. It‚Äôs certainly not a good idea to simply add things without review to a project‚Äôs production. Like if you got access to modify the institute‚Äôs website and you made your first changes and all of a sudden the website looks like this: It used to be subtle and light gray. I swear! Bet everybody on the team took notice of the new team member by then. In a feature branch workflow you would start from the latest production version. Remember, git is decentralized and you have all versions of your team‚Äôs project. Right at your fingertips on your local machine. Create a new branch named indicative of the feature you are looking to work on. git checkout -b colorways You are automatically being switched to the freshly created branch. Do your thing now. It could be just a single commit, or several commits by different persons. Once you are done, i.e., commited all changes, add your branch to the remote repository by pushing. git push -u origin colorways This will add a your branch called colorways to the remote repository. If you are on any major git platform with your project, it will come with a decent web GUI. Such a GUI is the most straight forward way to do the next step: get your Pull Request (PR) going. Github pull request dialog: Select the pull request, choose which branch you merge into which target branch. As you can see, git will check whether it is possible to merge automatically w/o interaction. Even if that is not possible, you can still issue the pull request. When you create the request you can also assign reviewers, but you could also do so at a later stage. Note, even after a PR was issued you can continue to add commits to the branch about to be merged. As long as you do not merge the branch through the Pull Request, commits are added to the branch. In other words your existing PR gets updated. This is a very natural way to account for reviewer comments. Pro-Tipp: Use commit messages like ‚Äòadded join to SQL query, closes #3‚Äô. The key word ‚Äòcloses‚Äô or ‚Äòfixes‚Äô, will automatically close issues referred to when merged into the main branch. Once the merge is done, all your changes are in the main branch and you and everyone else can pull the main branch that now contains your new feature. Yay! 4.3 Project Management Basics The art of stress free productivity as I once called it in ‚Äô10 blog post, has put a number of gurus on the map and whole strand of literature to our bookshelves. So rather than adding to that, I would like to extract a healthy, best-of-breed type of dose here. The following few paragraphs do not intend to be comprehensive ‚Äì not even for the scope of software projects, but inspirational. In tje software development startup community, the waterfall approach became synonym to conservative, traditional and ancient: Overspecification in advance of the project, premature optimization and a lawsuit over expectations that weren‚Äôt met. Though waterfall project may be better than their reputation and specifications should not be too detailed and rigid. Many software projects are rather organized in agile fashion with SCRUM and KANBAN being the most popular derivatives. Because empirical academic projects have a lot in common with software projects inasmuch that there is a certain expectation and quality control, but the outcome is not known in advance. Essentially in agile project management you roughly define an outcome along the lines of a minimum viable product (MVP). That way you do not end up with nothing after a year of back and forth. During the implementation you‚Äôd meet regularly, let‚Äôs say every 10 days, to discuss development since the last meet and what short term plans for the next steps ahead. The team picks splits work into task items on the issue tracker and assigns them. Solution to problems will only be sketched out and discussed bilaterally or in small groups. By defining the work package for only a short timespan, the team stays flexible. In professional setups agile development is often strictly implemented and makes use of sophisticated systems of roles that developers and project managers can get certified for. Major git platforms ship with a decent, carpentry level project management project management GUI. The issue tracker is at the core of this. If you use it the minimal way, it‚Äôs simply a colorful to-do list. Yet, with a bit of inspiration and use of tags, comments and projects, an issue tracker can be a lot more The Github issue tracker (example from one of the course‚Äôs repositories) can be a lot more than a todo list. Swimlanes (reminiscant of a bird‚Äôs eye view of an Olympic pool) can be thought of columns that you have to walk through from left to right: To Do, Doing, Under Review, Done. (you can also customize the number and label of lanes and event associate actions with them, but let‚Äôs stick to those basic lanes in this section.) The idea is to use to keep track of the process and making the process transparent. GitHub‚Äôs web platform offers swimlanes to keep a better overview of issues being worked on. ```{, type=‚Äònote‚Äô} Tipp: No lane except ‚ÄòDone‚Äô should contain more than 5-6 issues. Doing so prevents clogging the lanes at particular stage which could potentially lead to negligent behavior, e.g., careless reviews. ``` 4.4 Testing When talking about development practices, testing can‚Äôt be missing. So, just you know that I thought of this, tbc ‚Ä¶ GitHub Desktop, Atlassian‚Äôs Source Tree and Tortoise are some of the most popular choices if you are not a console person.‚Ü© "],
["appendix.html", "Appendix Glossary", " Appendix Glossary Term Description API Application Programming Interface Deployment The art of delivering a piece software to production Fork a clone of a repository that you (usually) do not own. GUI Graphical User Interface IDE Integrated Development Environment Merge Request See Pull Request. OS Operating Systen OSS Open Source Software Pull Request (PR) Request to join a feature branch into another branch, e.g., main branch. Sometimes it‚Äôs also called merge request. Regular Expression Pattern to extract specific parts from a text, find stuff in a text. Reproducible Example A self-contained code example, including the data it needs to run. Stack selection of software used in a project SQL Structured Query Language Swimlanes (Online) Board of columns (lanes). Lanes progress from from left to right and carry issues. Virtual Machine (VM) A virtual computer hosted on your computer. Often used to run another OS inside your main OS for testing purposes. "]
]
